{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differences between Spark objects:\n",
    "<img alt=\"memory usage\" src=\"https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"type safety\" src=\"https://databricks.com/wp-content/uploads/2016/07/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use an RDD when:\n",
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a dataframe when:\n",
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "\n",
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n",
    "\n",
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p_df = pd.read_csv('users.csv', sep='|') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"users.csv\").map(lambda line: line.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"users.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively:\n",
    "from_pandas_df = spark.createDataFrame(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's already a DF, but this is the easy way to rename columns\n",
    "df = (spark.read.csv(\"users.csv\", sep=\"|\")\n",
    "           .toDF(\"id\", \"age\", \"gender\", \"occupation\", \"zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.where(\"occupation != 'other'\")\n",
    "      .groupby(\"occupation\")\n",
    "      .count()\n",
    "      .sort(\"count\", ascending=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.limit(5).toPandas()\n",
    "# df.head()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.agg(F.countDistinct('occupation')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use SQL to query\n",
    "df.createOrReplaceTempView('users')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT occupation, COUNT(*) as count\n",
    "FROM users\n",
    "GROUP BY occupation\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d94882a8-37b6-4d1c-9c36-1a1e1c2d63e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources\n",
    "This notebook contains for code samples for *Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a71c194-2c8f-492a-933a-449dc22b16ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### User Defined Functions\n",
    "While Apache Spark has a plethora of functions, the flexibility of Spark allows for data engineers and data scientists to define their own functions (i.e., user-defined functions or UDFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8abac084-292d-4713-bab5-73e87674b906",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "605947c6-6ea7-420a-aab0-2b5a6b901ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9513ca7c-aa22-47d3-ad31-b96c7b6cf8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Speeding up and Distributing PySpark UDFs with Pandas UDFs\n",
    "One of the previous prevailing issues with using PySpark UDFs was that it had slower performance than Scala UDFs.  This was because the PySpark UDFs required data movement between the JVM and Python working which was quite expensive.   To resolve this problem, pandas UDFs (also known as vectorized UDFs) were introduced as part of Apache Spark 2.3. It is a UDF that uses Apache Arrow to transfer data and utilizes pandas to work with the data. You simplify define a pandas UDF using the keyword pandas_udf as the decorator or to wrap the function itself.   Once the data is in Apache Arrow format, there is no longer the need to serialize/pickle the data as it is already in a format consumable by the Python process.  Instead of operating on individual inputs row-by-row, you are operating on a pandas series or dataframe (i.e. vectorized execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b2491c4-31a0-4fc2-b757-7e8a4d5a1100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# Create the pandas UDF for the cubed function \n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61eb36e7-c32c-457b-b902-c52bf8e6a0cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "497c7da2-fb79-4925-a21f-fea13579a654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Pandas series\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.apply(cubed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d99aa0fb-3b42-4e5b-a1bd-89e524682737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de7ca7e0-1224-47ba-a777-f0f0b8359620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7584badf-ac42-42af-b4fe-d5abb9192288",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Higher Order Functions in DataFrames and Spark SQL\n",
    "\n",
    "Because complex data types are an amalgamation of simple data types, it is tempting to manipulate complex data types directly. As noted in the post *Introducing New Built-in and Higher-Order Functions for Complex Data Types in Apache Spark 2.4* there are typically two solutions for the manipulation of complex data types.\n",
    "1. Exploding the nested structure into individual rows, applying some function, and then re-creating the nested structure as noted in the code snippet below (see Option 1)  \n",
    "1. Building a User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2944da83-3786-4014-b2a6-aead1bebc2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an array dataset\n",
    "arrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n",
    "\n",
    "# Create schema\n",
    "from pyspark.sql.types import *\n",
    "arraySchema = (StructType([\n",
    "      StructField(\"id\", IntegerType(), True), \n",
    "      StructField(\"values\", ArrayType(IntegerType()), True)\n",
    "      ]))\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d45834ca-e900-4133-959d-14ec97cbba22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Option 1: Explode and Collect\n",
    "In this nested SQL statement, we first `explode(values)` which creates a new row (with the id) for each element (`value`) within values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15b67b4e-491a-4a84-ac32-0eb3d4fcd123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT id, collect_list(value + 1) AS newValues\n",
    "  FROM  (SELECT id, explode(values) AS value\n",
    "        FROM table) x\n",
    " GROUP BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34fab5ae-9c75-4149-aa68-6b22986cc671",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Option 2: User Defined Function\n",
    "To perform the same task (adding a value of 1 to each element in `values`), we can also create a user defined function (UDF) that uses map to iterate through each element (`value`) to perform the addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86c8e52c-90d4-4316-9d9c-819ee4b21783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# Create UDF\n",
    "def addOne(values):\n",
    "    return [value + 1 for value in values]\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"plusOneIntPy\", addOne, ArrayType(IntegerType()))  \n",
    "\n",
    "# Query data\n",
    "spark.sql(\"SELECT id, plusOneIntPy(values) AS values FROM table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c122aa76-17bd-4570-b98d-647f2575b225",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Higher-Order Functions\n",
    "In addition to the previously noted built-in functions, there are high-order functions that take anonymous lambda functions as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e6565c8-9f40-407f-8566-7487b5bb081e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f0bb80db-3eb6-45f4-8672-0b5fb787c016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transform\n",
    "\n",
    "`transform(array<T>, function<T, U>): array<U>`\n",
    "\n",
    "The transform function produces an array by applying a function to each element of an input array (similar to a map function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff93cdd2-f5ab-4b1f-896c-e993cbb5e80a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  celsius,\n",
    "  transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "760940c5-1b37-40a2-8c93-d40cc90c9f65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filter\n",
    "\n",
    "`filter(array<T>, function<T, Boolean>): array<T>`\n",
    "\n",
    "The filter function produces an array where the boolean function is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "596a6360-8098-4a81-9918-650d2f6e3df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3cb18bb-6b0f-4a86-b607-36ac81ee5eff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exists\n",
    "\n",
    "`exists(array<T>, function<T, V, Boolean>): Boolean`\n",
    "\n",
    "The exists function returns true if the boolean function holds for any element in the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f138b45-77c2-47ad-a414-fb3815b519c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d981fc2a-698d-4f26-8690-3f87714ffb54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reduce\n",
    "\n",
    "`reduce(array<T>, B, function<B, T, B>, function<B, R>)`\n",
    "\n",
    "The reduce function reduces the elements of the array to a single value  by merging the elements into a buffer B using function<B, T, B> and by applying a finishing function<B, R> on the final buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82eec166-1d0f-4b7f-acc1-a7faf7fdfd77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    "       reduce(\n",
    "          celsius, \n",
    "          0, \n",
    "          (t, acc) -> t + acc, \n",
    "          acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "        ) as avgFahrenheit \n",
    "  FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ef06ec2-7f86-49d8-be5f-cf5dca016e44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrames and Spark SQL Common Relational Operators\n",
    "\n",
    "The power of Spark SQL is that it contains many DataFrame Operations (also known as Untyped Dataset Operations). \n",
    "\n",
    "For the full list, refer to [Spark SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html).\n",
    "\n",
    "In the next section, we will focus on the following common relational operators:\n",
    "* Unions and Joins\n",
    "* Windowing\n",
    "* Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d01daad1-6755-4ad5-b58b-a6b1bdcb8337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Set File Paths\n",
    "delays_path = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airports_path = \"/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain airports dataset\n",
    "airports = spark.read.options(header='true', inferSchema='true', sep='\\t').csv(airports_path)\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure Delays data\n",
    "delays = spark.read.options(header='true').csv(delays_path)\n",
    "delays = (delays\n",
    "          .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "          .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "delays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "# Create temporary small table\n",
    "foo = delays.filter(expr(\"\"\"\n",
    "            origin == 'SEA' AND \n",
    "            destination == 'SFO' AND \n",
    "            date like '01010%' AND \n",
    "            delay > 0\"\"\"))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23030d91-f0f6-466b-ae3a-61ca6ec06623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3e0caa99-f0e0-4fdd-bcab-dedee8be4c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9ffa649-56c9-47a4-8b8f-35c9f406ee1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM foo LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fde706fd-c039-4797-b017-d00bcfa8ea70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cf1aa56-3e9d-4472-a8aa-08e1c4b51b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union two tables\n",
    "bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09981cdb-802b-4597-b8c9-2d3fa6c712bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM bar \n",
    "WHERE origin = 'SEA' \n",
    "   AND destination = 'SFO' \n",
    "   AND date LIKE '01010%' \n",
    "   AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c6af40cb-5fe9-4ada-8a66-99f0f9eae309",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Joins\n",
    "By default, it is an `inner join`.  There are also the options: `inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti`.\n",
    "\n",
    "More info available at:\n",
    "* [PySpark Join](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a9e0050-fc6f-4e0a-b08e-c29b9f030d63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join Departure Delays data (foo) with flight info\n",
    "foo.join(\n",
    "  airports, \n",
    "  airports.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47ce9c99-fd1a-40f8-acc9-3e1b2cd687cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    "  FROM foo f\n",
    "  JOIN airports_na a\n",
    "    ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb7d5e48-c6f0-4b9c-96b7-5628d256c74b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Windowing Functions\n",
    "\n",
    "Great reference: [Introduction Windowing Functions in Spark SQL](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\n",
    "\n",
    "> At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the Frame. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6a71ec5-d014-45dd-9bd6-350be23faab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, sum(delay) as TotalDelays \n",
    "  FROM departureDelays \n",
    " WHERE origin IN ('SEA', 'SFO', 'JFK') \n",
    "   AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \n",
    " GROUP BY origin, destination\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8785d4df-6bde-4b17-8d17-9f03de84debf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What are the top three total delays destinations by origin city of SEA, SFO, and JFK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff3a61f2-9949-4d91-b5ed-d9d62902c951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, sum(TotalDelays) as TotalDelays\n",
    " FROM departureDelaysWindow\n",
    "WHERE origin = 'SEA'\n",
    "GROUP BY origin, destination\n",
    "ORDER BY sum(TotalDelays) DESC\n",
    "LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb9cec14-8fb2-4b34-bff5-e69f20b4177b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank \n",
    "  FROM ( \n",
    "     SELECT origin, destination, TotalDelays, dense_rank() \n",
    "       OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank \n",
    "       FROM departureDelaysWindow\n",
    "  ) t \n",
    " WHERE rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2246ab42-bd80-432f-b49a-cc3a560855a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modifications\n",
    "\n",
    "Another common DataFrame operation is to perform modifications to the DataFrame. Recall that the underlying RDDs are immutable (i.e. they do not change) to ensure there is data lineage for Spark operations. Hence while DataFrames themselves are immutable, you can modify them through operations that create a new, different DataFrame with different columns, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "497a711b-9d8f-4722-9321-d3caa55e8c5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "677cdbbc-086e-4565-a01e-5ba169fce69c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "edb017ed-78fe-4d57-a52a-86934d02041c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT *, CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END AS status FROM foo\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74c855ae-f0d3-414c-a4a9-3709a50e675c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f0a22ed3-fe26-4c4f-aab2-5066db5141dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e20f2751-60bf-4346-80f7-6d86bea0f059",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37e94e76-b5e2-4f91-b9b8-0413414af069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18614995-8403-4c87-a563-ca7fe4f87fb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pivoting\n",
    "Great reference [SQL Pivot: Converting Rows to Columns](https://databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "512a1f1e-eb59-4a1b-bb70-abcc794466fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay FROM departureDelays WHERE origin = 'SEA'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad0b27da-f6b2-4625-985c-c0119ed85610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB, 3 MAR)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed0ed30c-89e4-4557-ba12-b37bcda93c87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28e509a9-214b-43ff-90d0-83d6be14b5ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Rollup\n",
    "Refer to [What is the difference between cube, rollup and groupBy operators?](https://stackoverflow.com/questions/37975227/what-is-the-difference-between-cube-rollup-and-groupby-operators)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "5-1 Spark SQL & UDFs",
   "notebookOrigID": 1711869132946507,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
