{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preliminary-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legitimate-forty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.181:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ce67af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vertical-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = (spark.readStream.format('socket')\n",
    "                             .option('host', 'localhost')\n",
    "                             .option('port', 22223)\n",
    "                             .load())\n",
    "\n",
    "json_df = stream_df.selectExpr(\"CAST(value AS STRING) AS payload\")\n",
    "\n",
    "writer = (\n",
    "    json_df.writeStream\n",
    "           .queryName('askReddit')\n",
    "           .format('memory')\n",
    "           .outputMode('append')\n",
    ")\n",
    "\n",
    "streamer = writer.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electric-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    id|score|\n",
      "+------+-----+\n",
      "|tynl33| 5615|\n",
      "|ty9i6m| 2319|\n",
      "|ty2yxf| 1631|\n",
      "|tyncry|   32|\n",
      "|tybxkx|10303|\n",
      "|tylv56|  256|\n",
      "|tyllyi|   55|\n",
      "|tygbgf| 2903|\n",
      "|txzu59|40567|\n",
      "|tyg6nx|  112|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[id: string, score: int]\n",
      "+------+-----+\n",
      "|    id|score|\n",
      "+------+-----+\n",
      "|tynl33| 5615|\n",
      "|ty9i6m| 2319|\n",
      "|ty2yxf| 1631|\n",
      "|tyncry|   32|\n",
      "|tybxkx|10303|\n",
      "|tylv56|  256|\n",
      "|tyllyi|   55|\n",
      "|tygbgf| 2903|\n",
      "|txzu59|40567|\n",
      "|tyg6nx|  112|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[id: string, score: int]\n",
      "+------+-----+\n",
      "|    id|score|\n",
      "+------+-----+\n",
      "|tynl33| 5615|\n",
      "|ty9i6m| 2319|\n",
      "|ty2yxf| 1631|\n",
      "|tyncry|   32|\n",
      "|tybxkx|10303|\n",
      "|tylv56|  256|\n",
      "|tyllyi|   55|\n",
      "|tygbgf| 2903|\n",
      "|txzu59|40567|\n",
      "|tyg6nx|  112|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[id: string, score: int]\n",
      "+------+-----+\n",
      "|    id|score|\n",
      "+------+-----+\n",
      "|tynl33| 5615|\n",
      "|ty9i6m| 2319|\n",
      "|ty2yxf| 1631|\n",
      "|tyncry|   32|\n",
      "|tybxkx|10303|\n",
      "|tylv56|  256|\n",
      "|tyllyi|   55|\n",
      "|tygbgf| 2903|\n",
      "|txzu59|40567|\n",
      "|tyg6nx|  112|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[id: string, score: int]\n",
      "+------+-----+\n",
      "|    id|score|\n",
      "+------+-----+\n",
      "|tynl33| 5615|\n",
      "|ty9i6m| 2319|\n",
      "|ty2yxf| 1631|\n",
      "|tyncry|   32|\n",
      "|tybxkx|10303|\n",
      "|tylv56|  256|\n",
      "|tyllyi|   55|\n",
      "|tygbgf| 2903|\n",
      "|txzu59|40567|\n",
      "|tyg6nx|  112|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[id: string, score: int]\n",
      "streaming done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(5):\n",
    "    df = spark.sql(\"\"\"\n",
    "    SELECT get_json_object(payload, '$.id') AS id,\n",
    "           CAST(get_json_object(payload, '$.score') AS INTEGER) AS score\n",
    "    FROM askReddit\n",
    "    \"\"\")\n",
    "    \n",
    "    df.show(10)\n",
    "    \n",
    "    print(df)\n",
    "    time.sleep(5)\n",
    "    \n",
    "streamer.awaitTermination(timeout=10)\n",
    "print('streaming done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "permanent-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------------------+\n",
      "|    id|min(score)|max(score)|            pct_diff|\n",
      "+------+----------+----------+--------------------+\n",
      "|typux9|        20|        27|                0.35|\n",
      "|tyncry|        25|        32|                0.28|\n",
      "|tyllyi|        53|        65| 0.22641509433962265|\n",
      "|tyoc5e|        93|       112| 0.20430107526881722|\n",
      "|tyrdu4|        15|        18|                 0.2|\n",
      "|tylvsx|       387|       444| 0.14728682170542637|\n",
      "|tyhrvs|        69|        77| 0.11594202898550725|\n",
      "|tynl33|      5615|      6263| 0.11540516473731077|\n",
      "|tyqttl|        18|        20|  0.1111111111111111|\n",
      "|tyl5jf|       726|       801| 0.10330578512396695|\n",
      "|tyl7md|        94|       103| 0.09574468085106383|\n",
      "|tyk7qs|       191|       207| 0.08376963350785341|\n",
      "|tyg6nx|       108|       116| 0.07407407407407407|\n",
      "|tyk4k9|       131|       139|0.061068702290076333|\n",
      "|tylv56|       255|       269|0.054901960784313725|\n",
      "|tykh27|       703|       740| 0.05263157894736842|\n",
      "|tybl6v|       329|       345|  0.0486322188449848|\n",
      "|tygbgf|      2902|      2993| 0.03135768435561682|\n",
      "|ty9i6m|      2313|      2352|0.016861219195849545|\n",
      "|tybxkx|     10295|     10449|0.014958717824186498|\n",
      "|ty2yxf|      1628|      1645|0.010442260442260442|\n",
      "|ty8w6k|      8512|      8596|0.009868421052631578|\n",
      "|txz96c|      1850|      1862|0.006486486486486486|\n",
      "|ty7pux|     13753|     13833|0.005816912673598487|\n",
      "|txqwcp|     15157|     15173|0.001055617866332...|\n",
      "+------+----------+----------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "(\n",
    "    df.groupBy('id')\n",
    "      .agg(F.min('score'), F.max('score'))\n",
    "      .withColumn('pct_diff', (F.col('max(score)') - F.col('min(score)'))/F.col('min(score)'))\n",
    "      .orderBy('pct_diff', ascending=False)\n",
    ").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-blake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
