{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-green",
   "metadata": {},
   "source": [
    "Directions\n",
    "0. Download the file from the above site or that's attached here.  Save it in the same directory as your notebook using the default name Part1_crime_data.csv\n",
    "1. Specify the schema for the crime data set. (https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)\n",
    "2. Read the file using the schema definition\n",
    "3. Cache the DataFrame (https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/)\n",
    "4. Show the count of the rows\n",
    "5. Print the schema\n",
    "6. Display first 5 rows\n",
    "\n",
    "\n",
    "Answer following questions using PySpark\n",
    "1. What are distinct crime codes?\n",
    "2. Count the number of crimes by the crime codes and order by the resulting counts in descending order\n",
    "3. Which neighborhood had most crimes?\n",
    "4. Which month of the year had most crimes?\n",
    "5. What weapons were used? \n",
    "6. Which weapon was used the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "shaped-corner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1_Crime_data.csv        hw03-1-enkeboll.py\r\n",
      "Untitled.ipynb              hw03-2-enkeboll.py\r\n",
      "hw01-tolstoy-enkeboll.ipynb hw03-3-enkeboll.py\r\n",
      "hw01-tolstoy-enkeboll.py    yelp.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complimentary-least",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿X,Y,RowID,CrimeDateTime,CrimeCode,Location,Description,Inside_Outside,Weapon,Post,District,Neighborhood,Latitude,Longitude,GeoLocation,Premise,VRIName,Total_Incidents\r\n",
      "1428019.10487147,589532.731060804,1,2022/03/05 03:43:00+00,3NF,1800 FLEET ST,ROBBERY - STREET,,FIREARM,213,SOUTHEAST,FELLS POINT,39.2847,-76.5913,\"(39.2847,-76.5913)\",,,1\r\n",
      "1428019.10487147,589532.731060804,2,2022/03/05 03:43:00+00,3NF,1800 FLEET ST,ROBBERY - STREET,,FIREARM,213,SOUTHEAST,FELLS POINT,39.2847,-76.5913,\"(39.2847,-76.5913)\",,,1\r\n",
      "1428019.10487147,589532.731060804,3,2022/03/05 03:43:00+00,3NF,1800 FLEET ST,ROBBERY - STREET,,FIREARM,213,SOUTHEAST,FELLS POINT,39.2847,-76.5913,\"(39.2847,-76.5913)\",,,1\r\n",
      "1423024.46477252,572683.29122287,4,2022/03/05 04:30:00+00,3CF,100 E PATAPSCO AVE,ROBBERY - COMMERCIAL,,FIREARM,913,SOUTHERN,BROOKLYN,39.2385,-76.6092,\"(39.2385,-76.6092)\",,,1\r\n",
      "1400632.37017858,593904.722108476,5,2022/03/05 01:30:00+00,3AF,RD & KEVIN RD,ROBBERY - STREET,,FIREARM,822,SOUTHWEST,ROGNEL HEIGHTS,39.297,-76.688,\"(39.297,-76.688)\",,,1\r\n",
      "1428080.44697806,588476.72267982,6,2022/03/05 03:00:00+00,3AF,900 FELL ST,ROBBERY - STREET,,FIREARM,213,SOUTHEAST,FELLS POINT,39.2818,-76.5911,\"(39.2818,-76.5911)\",,,1\r\n",
      "1414280.17070409,592206.638800635,7,2022/03/05 01:30:00+00,6F,SARATOGA ST & N CALHOUN ST,LARCENY,,NA,711,WESTERN,FRANKLIN SQUARE,39.2922,-76.6398,\"(39.2922,-76.6398)\",,,1\r\n",
      "1415908.88866331,588352.223355187,8,2022/03/05 01:55:00+00,4B,1200 WASHINGTON BLVD,AGG. ASSAULT,,KNIFE,932,SOUTHERN,WASHINGTON VILLAGE/PIGTOWN,39.2816,-76.6341,\"(39.2816,-76.6341)\",,,1\r\n",
      "1416311.79621168,600774.306828942,9,2022/03/05 01:05:00+00,4E,2400 LAKEVIEW AVE,COMMON ASSAULT,,NA,133,CENTRAL,RESERVOIR HILL,39.3157,-76.6325,\"(39.3157,-76.6325)\",,,1\r\n"
     ]
    }
   ],
   "source": [
    "!head Part1_Crime_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "discrete-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  513757\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l < Part1_Crime_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "colonial-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "maritime-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.csv('Part1_Crime_data.csv', header=True, inferSchema=True, timestampFormat='yyyy/MM/dd HH:mm:ss+00').limit(5).toPandas()\n",
    "df = spark.read.csv('Part1_Crime_data.csv', header=True, inferSchema=True, timestampFormat='yyyy/MM/dd HH:mm:ss+00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "metric-pound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- RowID: integer (nullable = true)\n",
      " |-- CrimeDateTime: timestamp (nullable = true)\n",
      " |-- CrimeCode: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Inside_Outside: string (nullable = true)\n",
      " |-- Weapon: string (nullable = true)\n",
      " |-- Post: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- GeoLocation: string (nullable = true)\n",
      " |-- Premise: string (nullable = true)\n",
      " |-- VRIName: string (nullable = true)\n",
      " |-- Total_Incidents: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "french-wrestling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(X=1428019.10487147, Y=589532.731060804, RowID=1, CrimeDateTime=datetime.datetime(2022, 3, 5, 3, 43), CrimeCode='3NF', Location='1800 FLEET ST', Description='ROBBERY - STREET', Inside_Outside=None, Weapon='FIREARM', Post='213', District='SOUTHEAST', Neighborhood='FELLS POINT', Latitude=39.2847, Longitude=-76.5913, GeoLocation='(39.2847,-76.5913)', Premise=None, VRIName=None, Total_Incidents=1),\n",
       " Row(X=1428019.10487147, Y=589532.731060804, RowID=2, CrimeDateTime=datetime.datetime(2022, 3, 5, 3, 43), CrimeCode='3NF', Location='1800 FLEET ST', Description='ROBBERY - STREET', Inside_Outside=None, Weapon='FIREARM', Post='213', District='SOUTHEAST', Neighborhood='FELLS POINT', Latitude=39.2847, Longitude=-76.5913, GeoLocation='(39.2847,-76.5913)', Premise=None, VRIName=None, Total_Incidents=1),\n",
       " Row(X=1428019.10487147, Y=589532.731060804, RowID=3, CrimeDateTime=datetime.datetime(2022, 3, 5, 3, 43), CrimeCode='3NF', Location='1800 FLEET ST', Description='ROBBERY - STREET', Inside_Outside=None, Weapon='FIREARM', Post='213', District='SOUTHEAST', Neighborhood='FELLS POINT', Latitude=39.2847, Longitude=-76.5913, GeoLocation='(39.2847,-76.5913)', Premise=None, VRIName=None, Total_Incidents=1),\n",
       " Row(X=1423024.46477252, Y=572683.29122287, RowID=4, CrimeDateTime=datetime.datetime(2022, 3, 5, 4, 30), CrimeCode='3CF', Location='100 E PATAPSCO AVE', Description='ROBBERY - COMMERCIAL', Inside_Outside=None, Weapon='FIREARM', Post='913', District='SOUTHERN', Neighborhood='BROOKLYN', Latitude=39.2385, Longitude=-76.6092, GeoLocation='(39.2385,-76.6092)', Premise=None, VRIName=None, Total_Incidents=1),\n",
       " Row(X=1400632.37017858, Y=593904.722108476, RowID=5, CrimeDateTime=datetime.datetime(2022, 3, 5, 1, 30), CrimeCode='3AF', Location='RD & KEVIN RD', Description='ROBBERY - STREET', Inside_Outside=None, Weapon='FIREARM', Post='822', District='SOUTHWEST', Neighborhood='ROGNEL HEIGHTS', Latitude=39.297, Longitude=-76.688, GeoLocation='(39.297,-76.688)', Premise=None, VRIName=None, Total_Incidents=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spoken-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('Part1_Crime_data.csv', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "clean-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- RowID: integer (nullable = true)\n",
      " |-- CrimeDateTime: string (nullable = true)\n",
      " |-- CrimeCode: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Inside_Outside: string (nullable = true)\n",
      " |-- Weapon: string (nullable = true)\n",
      " |-- Post: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- GeoLocation: string (nullable = true)\n",
      " |-- Premise: string (nullable = true)\n",
      " |-- VRIName: string (nullable = true)\n",
      " |-- Total_Incidents: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "grave-weapon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[X: double, Y: double, RowID: int, CrimeDateTime: timestamp, CrimeCode: string, Location: string, Description: string, Inside_Outside: string, Weapon: string, Post: string, District: string, Neighborhood: string, Latitude: double, Longitude: double, GeoLocation: string, Premise: string, VRIName: string, Total_Incidents: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "consistent-committee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[X: double, Y: double, RowID: int, CrimeDateTime: timestamp, CrimeCode: string, Location: string, Description: string, Inside_Outside: string, Weapon: string, Post: string, District: string, Neighborhood: string, Latitude: double, Longitude: double, GeoLocation: string, Premise: string, VRIName: string, Total_Incidents: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strong-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513756"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "driven-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "signal-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|CrimeCode|\n",
      "+---------+\n",
      "|       3P|\n",
      "|       3K|\n",
      "|      3BJ|\n",
      "|       1A|\n",
      "|       3M|\n",
      "|       5F|\n",
      "|       4B|\n",
      "|       3B|\n",
      "|      3NF|\n",
      "|       7A|\n",
      "|      3EF|\n",
      "|       3N|\n",
      "|       5D|\n",
      "|       6K|\n",
      "|      3LO|\n",
      "|      3AF|\n",
      "|       7B|\n",
      "|      3GO|\n",
      "|     3AJF|\n",
      "|      8GV|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are distinct crime codes?\n",
    "df.select('CrimeCode').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "defined-basin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>RowID</th>\n",
       "      <th>CrimeDateTime</th>\n",
       "      <th>CrimeCode</th>\n",
       "      <th>Location</th>\n",
       "      <th>Description</th>\n",
       "      <th>Inside_Outside</th>\n",
       "      <th>Weapon</th>\n",
       "      <th>Post</th>\n",
       "      <th>District</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>GeoLocation</th>\n",
       "      <th>Premise</th>\n",
       "      <th>VRIName</th>\n",
       "      <th>Total_Incidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.428019e+06</td>\n",
       "      <td>589532.731061</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-05 03:43:00</td>\n",
       "      <td>3NF</td>\n",
       "      <td>1800 FLEET ST</td>\n",
       "      <td>ROBBERY - STREET</td>\n",
       "      <td>None</td>\n",
       "      <td>FIREARM</td>\n",
       "      <td>213</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>FELLS POINT</td>\n",
       "      <td>39.2847</td>\n",
       "      <td>-76.5913</td>\n",
       "      <td>(39.2847,-76.5913)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.428019e+06</td>\n",
       "      <td>589532.731061</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-03-05 03:43:00</td>\n",
       "      <td>3NF</td>\n",
       "      <td>1800 FLEET ST</td>\n",
       "      <td>ROBBERY - STREET</td>\n",
       "      <td>None</td>\n",
       "      <td>FIREARM</td>\n",
       "      <td>213</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>FELLS POINT</td>\n",
       "      <td>39.2847</td>\n",
       "      <td>-76.5913</td>\n",
       "      <td>(39.2847,-76.5913)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.428019e+06</td>\n",
       "      <td>589532.731061</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-03-05 03:43:00</td>\n",
       "      <td>3NF</td>\n",
       "      <td>1800 FLEET ST</td>\n",
       "      <td>ROBBERY - STREET</td>\n",
       "      <td>None</td>\n",
       "      <td>FIREARM</td>\n",
       "      <td>213</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>FELLS POINT</td>\n",
       "      <td>39.2847</td>\n",
       "      <td>-76.5913</td>\n",
       "      <td>(39.2847,-76.5913)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.423024e+06</td>\n",
       "      <td>572683.291223</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-03-05 04:30:00</td>\n",
       "      <td>3CF</td>\n",
       "      <td>100 E PATAPSCO AVE</td>\n",
       "      <td>ROBBERY - COMMERCIAL</td>\n",
       "      <td>None</td>\n",
       "      <td>FIREARM</td>\n",
       "      <td>913</td>\n",
       "      <td>SOUTHERN</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>39.2385</td>\n",
       "      <td>-76.6092</td>\n",
       "      <td>(39.2385,-76.6092)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.400632e+06</td>\n",
       "      <td>593904.722108</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-03-05 01:30:00</td>\n",
       "      <td>3AF</td>\n",
       "      <td>RD &amp; KEVIN RD</td>\n",
       "      <td>ROBBERY - STREET</td>\n",
       "      <td>None</td>\n",
       "      <td>FIREARM</td>\n",
       "      <td>822</td>\n",
       "      <td>SOUTHWEST</td>\n",
       "      <td>ROGNEL HEIGHTS</td>\n",
       "      <td>39.2970</td>\n",
       "      <td>-76.6880</td>\n",
       "      <td>(39.297,-76.688)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X              Y  RowID       CrimeDateTime CrimeCode  \\\n",
       "0  1.428019e+06  589532.731061      1 2022-03-05 03:43:00       3NF   \n",
       "1  1.428019e+06  589532.731061      2 2022-03-05 03:43:00       3NF   \n",
       "2  1.428019e+06  589532.731061      3 2022-03-05 03:43:00       3NF   \n",
       "3  1.423024e+06  572683.291223      4 2022-03-05 04:30:00       3CF   \n",
       "4  1.400632e+06  593904.722108      5 2022-03-05 01:30:00       3AF   \n",
       "\n",
       "             Location           Description Inside_Outside   Weapon Post  \\\n",
       "0       1800 FLEET ST      ROBBERY - STREET           None  FIREARM  213   \n",
       "1       1800 FLEET ST      ROBBERY - STREET           None  FIREARM  213   \n",
       "2       1800 FLEET ST      ROBBERY - STREET           None  FIREARM  213   \n",
       "3  100 E PATAPSCO AVE  ROBBERY - COMMERCIAL           None  FIREARM  913   \n",
       "4       RD & KEVIN RD      ROBBERY - STREET           None  FIREARM  822   \n",
       "\n",
       "    District    Neighborhood  Latitude  Longitude         GeoLocation Premise  \\\n",
       "0  SOUTHEAST     FELLS POINT   39.2847   -76.5913  (39.2847,-76.5913)    None   \n",
       "1  SOUTHEAST     FELLS POINT   39.2847   -76.5913  (39.2847,-76.5913)    None   \n",
       "2  SOUTHEAST     FELLS POINT   39.2847   -76.5913  (39.2847,-76.5913)    None   \n",
       "3   SOUTHERN        BROOKLYN   39.2385   -76.6092  (39.2385,-76.6092)    None   \n",
       "4  SOUTHWEST  ROGNEL HEIGHTS   39.2970   -76.6880    (39.297,-76.688)    None   \n",
       "\n",
       "  VRIName  Total_Incidents  \n",
       "0    None                1  \n",
       "1    None                1  \n",
       "2    None                1  \n",
       "3    None                1  \n",
       "4    None                1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "annoying-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|CrimeCode|crimeCodeCount|\n",
      "+---------+--------------+\n",
      "|       4E|         91822|\n",
      "|       6D|         68508|\n",
      "|       5A|         43956|\n",
      "|       7A|         40308|\n",
      "|       6J|         27670|\n",
      "|       6G|         26898|\n",
      "|       6E|         24310|\n",
      "|       6C|         23269|\n",
      "|       4C|         22455|\n",
      "|       5D|         14971|\n",
      "|      3AF|         14760|\n",
      "|       4B|         14501|\n",
      "|       4A|         13240|\n",
      "|       3B|         10735|\n",
      "|       4D|          7222|\n",
      "|       5B|          6481|\n",
      "|       9S|          5452|\n",
      "|       6F|          5058|\n",
      "|       5C|          4917|\n",
      "|       6B|          4106|\n",
      "+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of crimes by the crime codes and order by the resulting counts in descending order\n",
    "df.select('CrimeCode').groupBy('CrimeCode').agg(F.count('CrimeCode').alias('crimeCodeCount')).orderBy('crimeCodeCount', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "essential-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Neighborhood|count|\n",
      "+------------+-----+\n",
      "|    DOWNTOWN|17818|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which neighborhood had most crimes?\n",
    "df.select('Neighborhood').groupBy('Neighborhood').count().sort('count', ascending=False).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "trying-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|month(CrimeDateTime)|monthCount|\n",
      "+--------------------+----------+\n",
      "|                   8|     46322|\n",
      "|                  10|     46207|\n",
      "|                   7|     45982|\n",
      "|                   5|     45035|\n",
      "|                   6|     44989|\n",
      "|                   9|     44827|\n",
      "|                   1|     42225|\n",
      "|                  11|     42022|\n",
      "|                  12|     41593|\n",
      "|                   4|     40012|\n",
      "|                   3|     39002|\n",
      "|                   2|     35540|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which month of the year had most crimes?\n",
    "df.groupBy(F.month(df.CrimeDateTime)).agg(F.count('CrimeDateTime').alias('monthCount')).sort('monthCount', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "beneficial-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Weapon|\n",
      "+-------+\n",
      "|     NA|\n",
      "|   null|\n",
      "|  HANDS|\n",
      "|  KNIFE|\n",
      "|  OTHER|\n",
      "|   FIRE|\n",
      "|FIREARM|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What weapons were used?\n",
    "df.select('Weapon').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "thousand-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Weapon|\n",
      "+-------+\n",
      "|  HANDS|\n",
      "|  KNIFE|\n",
      "|  OTHER|\n",
      "|   FIRE|\n",
      "|FIREARM|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Weapon').dropna().where('Weapon!=\"NA\"').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "turned-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Weapon='FIREARM', count=46217)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which weapon was used the most?\n",
    "df.select('Weapon').dropna().where('Weapon!=\"NA\"').groupBy('Weapon').count().sort('count', ascending=False).limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "regular-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "mighty-quest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month(CrimeDateTime): int, count: bigint]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "statewide-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|month(CrimeDateTime)|count|\n",
      "+--------------------+-----+\n",
      "|                  12|41593|\n",
      "|                   1|42225|\n",
      "|                   6|44989|\n",
      "|                   3|39002|\n",
      "|                   5|45035|\n",
      "|                   9|44827|\n",
      "|                   4|40012|\n",
      "|                   8|46322|\n",
      "|                   7|45982|\n",
      "|                  10|46207|\n",
      "|                  11|42022|\n",
      "|                   2|35540|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "later-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [count#8336L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#8336L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#801]\n",
      "   +- *(2) HashAggregate(keys=[Weapon#992], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(Weapon#992, 200), ENSURE_REQUIREMENTS, [id=#797]\n",
      "         +- *(1) HashAggregate(keys=[Weapon#992], functions=[partial_count(1)])\n",
      "            +- *(1) Filter ((isnotnull(Weapon#992) AND AtLeastNNulls(n, Weapon#992)) AND NOT (Weapon#992 = NA))\n",
      "               +- InMemoryTableScan [Weapon#992], [isnotnull(Weapon#992), AtLeastNNulls(n, Weapon#992), NOT (Weapon#992 = NA)]\n",
      "                     +- InMemoryRelation [X#984, Y#985, RowID#986, CrimeDateTime#987, CrimeCode#988, Location#989, Description#990, Inside_Outside#991, Weapon#992, Post#993, District#994, Neighborhood#995, Latitude#996, Longitude#997, GeoLocation#998, Premise#999, VRIName#1000, Total_Incidents#1001], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- FileScan csv [X#984,Y#985,RowID#986,CrimeDateTime#987,CrimeCode#988,Location#989,Description#990,Inside_Outside#991,Weapon#992,Post#993,District#994,Neighborhood#995,Latitude#996,Longitude#997,GeoLocation#998,Premise#999,VRIName#1000,Total_Incidents#1001] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/enkeboll/code/UMBC/data603-sp22/homework/Part1_Crime_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:double,Y:double,RowID:int,CrimeDateTime:timestamp,CrimeCode:string,Location:string,Descr...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Weapon').dropna().where('Weapon!=\"NA\"').groupBy('Weapon').count().sort('count', ascending=False).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "massive-employer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/enkeboll/code/UMBC/data603-sp22/homework/spark-warehouse')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "joined-princess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "split-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(X=1428019.10487147, Y=589532.731060804, RowID=1, CrimeDateTime=datetime.datetime(2022, 3, 5, 3, 43), CrimeCode='3NF', Location='1800 FLEET ST', Description='ROBBERY - STREET', Inside_Outside=None, Weapon='FIREARM', Post='213', District='SOUTHEAST', Neighborhood='FELLS POINT', Latitude=39.2847, Longitude=-76.5913, GeoLocation='(39.2847,-76.5913)', Premise=None, VRIName=None, Total_Incidents=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "automatic-savannah",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o476.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:550)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:753)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:731)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:626)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 127.0 failed 1 times, most recent failure: Lost task 7.0 in stage 127.0 (TID 3178) (192.168.1.180 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:165)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:209)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:208)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:464)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 35 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:165)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:209)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:208)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:464)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-fff8a4afd4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'crime_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o476.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:550)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:753)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:731)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:626)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 127.0 failed 1 times, most recent failure: Lost task 7.0 in stage 127.0 (TID 3178) (192.168.1.180 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:165)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:209)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:208)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:464)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 35 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.int96RebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:165)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:209)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:208)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:158)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:464)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:148)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\n"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable('crime_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"/tmp/data/parquet/df_parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
