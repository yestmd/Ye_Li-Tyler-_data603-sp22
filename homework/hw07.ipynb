{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14a40c0-8b9c-49ee-92c4-527abd1ced82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c148f428-cfaf-40ed-9353-0df12c4e99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.option(\"header\", \"true\").csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbf8d17-ebcb-446b-ad8d-40037588849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|Date      |Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|2008-12-01|Albury  |13.4   |22.9   |0.6     |NA         |NA      |W          |44           |W         |WNW       |20          |24          |71         |22         |1007.7     |1007.1     |8       |NA      |16.9   |21.8   |No       |No          |\n",
      "|2008-12-02|Albury  |7.4    |25.1   |0       |NA         |NA      |WNW        |44           |NNW       |WSW       |4           |22          |44         |25         |1010.6     |1007.8     |NA      |NA      |17.2   |24.3   |No       |No          |\n",
      "|2008-12-03|Albury  |12.9   |25.7   |0       |NA         |NA      |WSW        |46           |W         |WSW       |19          |26          |38         |30         |1007.6     |1008.7     |NA      |2       |21     |23.2   |No       |No          |\n",
      "|2008-12-04|Albury  |9.2    |28     |0       |NA         |NA      |NE         |24           |SE        |E         |11          |9           |45         |16         |1017.6     |1012.8     |NA      |NA      |18.1   |26.5   |No       |No          |\n",
      "|2008-12-05|Albury  |17.5   |32.3   |1       |NA         |NA      |W          |41           |ENE       |NW        |7           |20          |82         |33         |1010.8     |1006       |7       |8       |17.8   |29.7   |No       |No          |\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab35dbaf-f0cc-454f-99ef-2505a536eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=df.toPandas().columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809e08af-7560-4592-8ff6-9524aa08b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8dd1d46-4f72-4b92-91f8-4a365cfcc92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(RainTomorrow='NA'), Row(RainTomorrow='No'), Row(RainTomorrow='Yes')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check rain tomorrow column value\n",
    "df.select (\"RainTomorrow\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ae23a0-77c7-4a48-bf70-4ce522b4277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclue the NA value row in the raintomorrow\n",
    "df_clean=df.where(df[\"RainTomorrow\"] != \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1e1b15-9a37-4562-a4be-22374cfae809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(RainTomorrow='No'), Row(RainTomorrow='Yes')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check result\n",
    "df_clean.select (\"RainTomorrow\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f0592a-aacb-4618-92ab-17ca6d105c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e83447-e5ee-403f-9753-8c6959c3ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(8018,[3035,3449,...|  0.0|\n",
      "|(8018,[3036,3449,...|  0.0|\n",
      "|(8018,[3009,3449,...|  0.0|\n",
      "|(8018,[3010,3449,...|  0.0|\n",
      "|(8018,[3011,3449,...|  0.0|\n",
      "|(8018,[3012,3449,...|  0.0|\n",
      "|(8018,[3013,3449,...|  0.0|\n",
      "|(8018,[3014,3449,...|  0.0|\n",
      "|(8018,[3037,3449,...|  1.0|\n",
      "|(8018,[3038,3449,...|  0.0|\n",
      "|(8018,[3015,3449,...|  1.0|\n",
      "|(8018,[3016,3449,...|  1.0|\n",
      "|(8018,[3017,3449,...|  1.0|\n",
      "|(8018,[3018,3449,...|  0.0|\n",
      "|(8018,[3019,3449,...|  0.0|\n",
      "|(8018,[3020,3449,...|  1.0|\n",
      "|(8018,[3021,3449,...|  1.0|\n",
      "|(8018,[3022,3449,...|  0.0|\n",
      "|(8018,[3023,3449,...|  0.0|\n",
      "|(8018,[3024,3449,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formula = RFormula(\n",
    "    formula=\"RainTomorrow ~ .\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(df_clean).transform(df_clean)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175c32a-1169-4545-971e-a1b0aaa3e266",
   "metadata": {},
   "source": [
    "1. split data, 2. training data clean and convert 3. parameter grid, use pipeline to training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763469d1-36ef-4e1d-b75c-10271a11cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff16b4e9-2cef-4584-a39a-e70dd73d0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(output)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = (VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=2).fit(output))\n",
    "\n",
    "(trainingData, testData) = output.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd011a4-550e-4360-88de-eef35979345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc837f2-76eb-4a1c-b53e-bb53d0166f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa5c554-4023-44b6-8aa7-65fa03124bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    .addGrid(dt.maxBins, [5, 10, 15]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.2, 0.4]) \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d019f8a-9bd3-4da9-a8b2-614509685651",
   "metadata": {},
   "source": [
    "4. coress-validata with 4 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ece1067-a006-43bc-8e8f-bf76707d61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv= CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=grid, numFolds=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af840614-da4d-4738-8e87-718b9b6d3754",
   "metadata": {},
   "source": [
    "5. model training ~ take long time here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ff0b2a-83e9-4058-a7e6-8fa1da17ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "936691d6-fb14-4545-aa02-89af5d2eb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metric = evaluator.evaluate(cvModel.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3acb5ea2-73ea-423d-9993-6b367089d199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985231329180373"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f57833-c52b-4e1f-8710-785b91eb0732",
   "metadata": {},
   "source": [
    "5. print parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7804af45-200f-4173-89ab-0a9e3823c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexerModel: uid=StringIndexer_a41e01018192, handleInvalid=error\n",
      "VectorIndexerModel: uid=VectorIndexer_20a1f9359598, numFeatures=8018, handleInvalid=error\n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_34defdf7a871, depth=7, numNodes=185, numClasses=2, numFeatures=8018\n"
     ]
    }
   ],
   "source": [
    "best_Model = cvModel.bestModel\n",
    "\n",
    "    #applicable to your model to pull list of all stages\n",
    "for x in range(len(best_Model.stages)):\n",
    "    print(best_Model.stages[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1167f892-98e6-49b1-8731-3de229fcf53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impurity': 'gini', 'maxBins': 5, 'minInfoGain': 0.0, 'maxDepth': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_model = best_Model.stages[-1]._java_obj\n",
    "{param.name: java_model.getOrDefault(java_model.getParam(param.name)) \n",
    "    for param in grid[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de527d-8dd2-4f0f-a23b-77caf7b13e10",
   "metadata": {},
   "source": [
    "6. ROC Curve and Area under Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff8beb64-1a2c-46c0-9249-508d17be29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497516f9-088f-4b3d-b284-447fbcf69e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = best_Model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac29eb-66de-493d-9e68-5df0fb101d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels.select(\"prediction\",\"indexedLabel\").rdd)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5212035-6032-4478-b9d9-454ab10b1ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
